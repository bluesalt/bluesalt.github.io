<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Tag: avro | Jeff Li]]></title>
  <link href="http://bluesalt.github.io/tags/avro/atom.xml" rel="self"/>
  <link href="http://bluesalt.github.io/"/>
  <updated>2016-04-12T21:36:18+08:00</updated>
  <id>http://bluesalt.github.io/</id>
  <author>
    <name><![CDATA[Jeff Li]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Avro Cookbook : Part III]]></title>
    <link href="http://bluesalt.github.io/blog/2014/04/08/avro-cookbook-part-iii/"/>
    <updated>2014-04-08T17:24:45+08:00</updated>
    <id>http://bluesalt.github.io/blog/2014/04/08/avro-cookbook-part-iii</id>
    <content type="html"><![CDATA[<ul id="markdown-toc">
  <li><a href="#recipe-6-serialize-data-as-json-data">Recipe 6: Serialize data as JSON data</a></li>
  <li><a href="#recipe-7-deserialize-json-data">Recipe 7: Deserialize JSON data</a></li>
  <li><a href="#recipe-8-serialize-array-in-json">Recipe 8: Serialize array in JSON</a></li>
  <li><a href="#recipe-9-deserialize-json-array-data">Recipe 9: Deserialize JSON array data</a></li>
  <li><a href="#recipe-10-deserialize-data-stream">Recipe 10: Deserialize data stream</a></li>
  <li><a href="#summary">Summary</a></li>
</ul>

<h2 id="recipe-6-serialize-data-as-json-data">Recipe 6: Serialize data as JSON data</h2>
<p>In <a href="http://jeffli.me/blog/2014/02/06/avro-cookbook-part-i/">Avro Cookbook : part I</a>, if you open the file <code>/tmp/log</code> created by recipe 3, you would find that it is definitely not a human readable text format. Avro provides the encoder/decoder mechanism which helps to serial the data to text format as JSON data.
<!--more-->
 Actually, if I want to serial the POJOs to JSON data, I would rather use <a href="https://code.google.com/p/google-gson/">Google Gson</a>. Anyway, this is a post about Avro, right ?</p>

<p>```java
@Test
public void testSerializeToJson() throws Exception {
   ByteArrayOutputStream outputStream = new ByteArrayOutputStream();
   Schema schema = ReflectData.get().getSchema(LogEntry3.class);</p>

<p>Encoder encoder = new EncoderFactory().jsonEncoder(schema, outputStream);
   DatumWriter<logentry3> writer = new ReflectDatumWriter&lt;&gt;(schema);</logentry3></p>

<p>LogEntry3 entry1 = new LogEntry3(“Jeff”, “readme.md”, “192.168.4.1”);
   LogEntry3 entry2 = new LogEntry3(“John”, “readme.txt”, “192.168.4.2”);</p>

<p>writer.write(entry1, encoder);
   writer.write(entry2, encoder);
   encoder.flush();</p>

<p>System.out.println(new String(outputStream.toByteArray()));
}
```
Refer to the <a href="http://localhost:4000/blog/2014/04/05/avro-cookbook-part-ii/">Avro Cookbook : Part II</a> for the what class <code>LogEntry3</code> looks like. Here is the output:</p>

<p><code>json
{"name":"Jeff","resource":"readme.md","ip":"192.168.4.1"}
{"name":"John","resource":"readme.txt","ip":"192.168.4.2"}
</code></p>

<h2 id="recipe-7-deserialize-json-data">Recipe 7: Deserialize JSON data</h2>

<p>```java
@Test
public void testDeserializeFromJson() throws Exception {
   String input = “{"name":"Jeff","resource":"readme.md","ip":"192.168.4.1"}” +
           “{"name":"John","resource":"readme.txt","ip":"192.168.4.2"}”;</p>

<p>ByteArrayInputStream inputStream = new ByteArrayInputStream(input.getBytes());
   Schema schema = ReflectData.get().getSchema(LogEntry3.class);</p>

<p>JsonDecoder decoder = new DecoderFactory().jsonDecoder(schema, inputStream);
   DatumReader<genericrecord> reader = new GenericDatumReader&lt;&gt;(schema);
   GenericRecord entry;
   while (true) {
      try {
         entry = reader.read(null, decoder);
         System.out.println(entry);
      } catch (EOFException exception) {
         break;
      }
   }
}
```
If you look at the code carefully, you will find several interesting things. </genericrecord></p>

<p>First, there is no explicit separator between every JSON record. That means the Avro JSON decoder can decode the JSON data in the form of <strong>stream</strong>. This could be very helpful when you have to deserialize a whole bunch of JSON records without any explicit separator between records.</p>

<p>Second, when parsing the JSON data, the generic reader <code>DatumReader&lt;GenericRecord&gt;</code> instead of specific reader <code>DatumReader&lt;LogEntry3&gt;</code> is used. I tried to use the specific reader but it was not able to work with the error:</p>
<pre>
org.apache.avro.AvroTypeException: Expected start-union. Got VALUE_STRING
	at org.apache.avro.io.JsonDecoder.error(JsonDecoder.java:697)
	at org.apache.avro.io.JsonDecoder.readIndex(JsonDecoder.java:441)
	at org.apache.avro.io.ResolvingDecoder.doAction(ResolvingDecoder.java:229)
	at org.apache.avro.io.parsing.Parser.advance(Parser.java:88)
	at org.apache.avro.io.ResolvingDecoder.readIndex(ResolvingDecoder.java:206)
	at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:155)
	at org.apache.avro.generic.GenericDatumReader.readField(GenericDatumReader.java:193)
	at org.apache.avro.reflect.ReflectDatumReader.readField(ReflectDatumReader.java:230)
	at org.apache.avro.generic.GenericDatumReader.readRecord(GenericDatumReader.java:183)
	at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:151)
	at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:142)
</pre>
<p>It turned out that the Avro JSON decoder can only parse JSON data like this</p>

<p><code>json
{"name":"Jeff","resource":{"string":"readme.md"},"ip":{"string":"192.168.4.1"}}
{"name":"John","resource":{"string":"readme.txt"},"ip":{"string":"192.168.4.2"}}
</code>
but will not work on data like this</p>

<p><code>json
{"name": "Jeff", "resource": "readme.md", "ip": "192.168.4.1"}
{"name": "John", "resource": "readme.txt", "ip": "192.168.4.2"}
</code>
After googling the issue, I found this: <a href="http://grokbase.com/t/avro/user/1345c5t7h1/issue-writing-union-in-avro">Issue writing union in avro</a>. So my suggestion is that don’t use Avro for json serialization and deserialization if you have other choices. </p>

<h2 id="recipe-8-serialize-array-in-json">Recipe 8: Serialize array in JSON</h2>

<p>```java
@Test
public void testSerializeArray() throws IOException {
   Schema schema = Schema.createArray(ReflectData.get().getSchema(LogEntry3.class));
   GenericData.Array<logentry3> logs = new GenericData.Array&lt;&gt;(2, schema);
   logs.add(new LogEntry3("Jeff", "readme.md", "192.168.5.1"));
   logs.add(new LogEntry3("John", "readme.txt", "192.168.5.2"));</logentry3></p>

<p>ByteArrayOutputStream outputStream = new ByteArrayOutputStream();
   Encoder encoder = new EncoderFactory().jsonEncoder(schema, outputStream);
   DatumWriter&lt;GenericData.Array<logentry3>&gt; writer = new ReflectDatumWriter&lt;&gt;(schema);
   writer.write(logs, encoder);
   encoder.flush();</logentry3></p>

<p>System.out.println(new String(outputStream.toByteArray()));
}
``` 
In the code above, not code generation is required, the <code>LogEntry3</code> is just a normal Java class. The serialized data would be</p>

<p><code>json
[{"name":"Jeff","resource":"readme.md","ip":"192.168.5.1"},{"name":"John","resource":"readme.txt","ip":"192.168.5.2"}]
</code>
However, if the type of element of the array is generated from schema defintion(See <a href="http://localhost:4000/blog/2014/02/06/avro-cookbook-part-i/">Recipe 2</a>), the output would be different:</p>

<p><code>json
[{"name":"Jeff","resource":{"string":"readme.md"},"ip":{"string":"192.168.5.1"}},{"name":"John","resource":{"string":"readme.txt"},"ip":{"string":"192.168.5.2"}}]
</code>
Still, I will not choose Avro to serialize data to JSON if I can use gson. However, this recipe is intended to demonstrate how to serialize a array without code generation from predefined schema. </p>

<h2 id="recipe-9-deserialize-json-array-data">Recipe 9: Deserialize JSON array data</h2>

<p><code>java
@Test
public void testDeserializeArray() throws Exception {
   Schema schema = Schema.createArray(ReflectData.get().getSchema(LogEntry3.class));
   String input = "[{\"name\":\"Jeff\",\"resource\":\"readme.md\",\"ip\":\"192.168.5.1\"},{\"name\":\"John\",\"resource\":\"readme.txt\",\"ip\":\"192.168.5.2\"}]";
   Decoder decoder = new DecoderFactory().jsonDecoder(schema, input);
   DatumReader&lt;GenericData.Array&lt;GenericData.Record&gt;&gt; reader = new GenericDatumReader&lt;&gt;(schema);
   GenericData.Array&lt;GenericData.Record&gt; logs = reader.read(null, decoder);
   GenericData.Record entry = logs.get(0);
   System.out.println(entry);
}
</code>
Note, we can use only generic reader, have not figured out how to use a specific reader yet. Maybe it would be ok for the binary encoder to use specific reader.</p>

<h2 id="recipe-10-deserialize-data-stream">Recipe 10: Deserialize data stream</h2>
<p>The word <strong>stream</strong> means that the size of the source data is unknown. For example. if you want to serialize data like this</p>
<pre>
[
   {
      "name":"Jeff",
      "resource":"readme.md",
      "ip":"192.168.5.1"
   },
   {
      "name":"John",
      "resource":"readme.txt",
      "ip":"192.168.5.2"
   }
][
   {
      "name":"Joe",
      "resource":"readme.md",
      "ip":"192.168.5.3"
   },
   {
      "name":"James",
      "resource":"readme.txt",
      "ip":"192.168.5.4"
   }
]
</pre>
<p>The data have several unusual characteristics due to which gson can not be simply applied. 
 * The whole data is not a valid JSON. Instead it is JSON records set. Besides, the size of data could be very large.
 * There is no explicit separator between records. If the records are separated by character such as <code>\n</code>, then one record can be read and parsed easily. </p>

<p>The format of the data is so poor which should be avoided in practice. However, sometimes, we ourselves are just the data consumers. We can parse the data with Avro like this:</p>

<p>```java
@Test
public void testDeserializeStream() throws Exception {
   Schema schema = Schema.createArray(ReflectData.get().getSchema(LogEntry3.class));
   String input = “[{“name”:”Jeff”,”resource”:”readme.md”,”ip”:”192.168.5.1”},{“name”:”John”,”resource”:”readme.txt”,”ip”:”192.168.5.2”}][{“name”:”Joe”,”resource”:”readme.md”,”ip”:”192.168.5.3”},{“name”:”James”,”resource”:”readme.txt”,”ip”:”192.168.5.4”}]
“;
   Decoder decoder = new DecoderFactory().jsonDecoder(schema, input);
   DatumReader&lt;GenericData.Array<logentry3>&gt; reader = new GenericDatumReader&lt;&gt;(schema);
   GenericData.Array<logentry3> parsedRecords;</logentry3></logentry3></p>

<p>int count = 0;
   while (true) {
      try {
         // We can iterate the parsedRecords to get every individual record
         parsedRecords = reader.read(null, decoder);
         count += parsedRecords.size();
      } catch (EOFException e) {
         break;
      }
   }</p>

<p>Assert.assertEquals(count, 4);
}
```</p>

<h2 id="summary">Summary</h2>
<p>This post is the last part of the three part series on Avro. To be honest, I myself use Avro rarely and I am not a Avro expert, thus the code examples may not be the most appropriate. They are just intended to help the starters to play with Avro quickly. Anyone who uses Avro frequently should spend more time on looking deeper into the Avro framework. It would be great if they are helpful to you!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Avro Cookbook : Part II]]></title>
    <link href="http://bluesalt.github.io/blog/2014/04/05/avro-cookbook-part-ii/"/>
    <updated>2014-04-05T18:31:35+08:00</updated>
    <id>http://bluesalt.github.io/blog/2014/04/05/avro-cookbook-part-ii</id>
    <content type="html"><![CDATA[<ul id="markdown-toc">
  <li><a href="#recipe-5-serialize-data-without-code-generation">Recipe 5: Serialize data without Code Generation</a>    <ul>
      <li><a href="#parse-schema-from-string">Parse Schema from String</a></li>
      <li><a href="#parse-schema-from-disk-file">Parse Schema from Disk File</a></li>
      <li><a href="#parse-schema-from-existing-java-class">Parse Schema from Existing Java Class</a></li>
    </ul>
  </li>
</ul>

<h2 id="recipe-5-serialize-data-without-code-generation">Recipe 5: Serialize data without Code Generation</h2>
<p>In formal recipes, before using Avro to serialize/deserialize data, schema files have to be defined to be leveraged by Avro code generation facility to generate the Java classes. This is also recommended when using Avro in Java. However, it is not required. Actually, you can parse the schema on the fly without code generation.
<!--more--></p>

<h3 id="parse-schema-from-string">Parse Schema from String</h3>
<p>The schema looks like this:</p>

<p><code>json
{
 "namespace": "me.jeffli.avrosamples.model",
 "type": "record",
 "name": "LogEntry2",
 "fields": [
     {"name": "name", "type": "string"},
     {"name": "resource",  "type": ["string", "null"]},
     {"name": "ip", "type": ["string", "null"]}
 ]
}
</code></p>

<p>Define the schema as a Java String:</p>
<pre>
String schemaDesc = "{\n" +
           " \"namespace\": \"me.jeffli.avrosamples.model\",\n" +
           " \"type\": \"record\",\n" +
           " \"name\": \"LogEntry2\",\n" +
           " \"fields\": [\n" +
           "     {\"name\": \"name\", \"type\": \"string\"},\n" +
           "     {\"name\": \"resource\",  \"type\": [\"string\", \"null\"]},\n" +
           "     {\"name\": \"ip\", \"type\": [\"string\", \"null\"]}\n" +
           " ]\n" +
           "}";
</pre>

<p>Then the code to serialize the data would be this:</p>

<p>```java
@Test
public void testSerializeOnTheFly() throws IOException {
   Schema schema = new Schema.Parser().parse(schemaDesc);
   GenericRecord entry1 = new GenericData.Record(schema);
   entry1.put(“name”, “Jeffrey”);
   entry1.put(“resource”, “README”);
   entry1.put(“ip”, “192.168.2.1”);</p>

<p>GenericRecord entry2 = new GenericData.Record(schema);
   entry2.put(“name”, “Johnson”);
   entry2.put(“resource”, “readme.markdown”);
   entry2.put(“ip”, “192.168.2.2”);</p>

<p>DatumWriter<genericrecord> datumWriter = new GenericDatumWriter&lt;&gt;(schema);
   DataFileWriter<genericrecord> dataFileWriter = new DataFileWriter&lt;&gt;(datumWriter);
   File file = new File("/tmp/log2");
   dataFileWriter.create(schema, file);</genericrecord></genericrecord></p>

<p>dataFileWriter.append(entry1);
   dataFileWriter.append(entry2);
   dataFileWriter.close();
}
```
From the example, we can see that we don’t need to define any external schema file and no external Java classed are generated.</p>

<h3 id="parse-schema-from-disk-file">Parse Schema from Disk File</h3>
<p>In the above example, we parse the schema from String. Since the schema is defined with JSON language, it is cumbersome to define the schema as a Java String. Fortunately Avro Schema.Paser also provides other API to parse the schema from disk file or existing Java class :</p>

<p><code>java
Schema schema = new Schema.Parser().parse(new File("src/test/resources/LogEntry2.avsc"));
</code></p>

<h3 id="parse-schema-from-existing-java-class">Parse Schema from Existing Java Class</h3>
<p>Per the JSON Schema definition, a equivalent Java class would look like this:</p>

<p>```java
public class LogEntry3 {
   private String name;
   private String resource;
   private String ip;</p>

<p>public LogEntry3(String name, String resource, String ip) {
      this.name = name;
      this.resource = resource;
      this.ip = ip;
   }</p>

<p>public String getName() {
      return name;
   }</p>

<p>public void setName(String name) {
      this.name = name;
   }</p>

<p>public String getResource() {
      return resource;
   }</p>

<p>public void setResource(String resource) {
      this.resource = resource;
   }</p>

<p>public String getIp() {
      return ip;
   }</p>

<p>public void setIp(String ip) {
      this.ip = ip;
   }
}
```
Then the Schema can be fetched easily:</p>

<p><code>java
Schema schema = ReflectData.get().getSchema(LogEntry3.class);
</code></p>

<p>What is more, you can use ReflectDatumWriter to append specific type objects to the target:</p>

<p>```java
@Test
public void testSerializeData() throws IOException {
   Schema schema = ReflectData.get().getSchema(LogEntry3.class);
   File file = new File(“/tmp/log3”);
   LogEntry3 entry1 = new LogEntry3(“Jeff”, “readme.txt”, “192.168.3.1”);
   LogEntry3 entry2 = new LogEntry3(“John”, “readme.md”, “192.168.3.2”);</p>

<p>ReflectDatumWriter<logentry3> reflectDatumWriter = new ReflectDatumWriter&lt;&gt;(schema);
   DataFileWriter<logentry3> writer = new DataFileWriter&lt;&gt;(reflectDatumWriter).create(schema, file);
   writer.append(entry1);
   writer.append(entry2);
   writer.close();
}
```
## Recipe 6: Deserialize data without Code Generation
Deserializing data without code generation is pretty easy. The only difference with Recipe 4 is how it get the **schema**. Thus the ways to fetch schemas in Recipe 5 are also applicable in this recipe. Here is only the example to load schema from disk file. I am pretty sure that you can finish the rest code. It should be noted that if the schema is parsed on the fly without any code generation, then when deserializing the data, you can only use the generic datum reader even if you attempt with **ReflectDatumReader**. </logentry3></logentry3></p>

<p><code>java
@Test(dependsOnMethods = "testSerializeOnTheFly")
public void testDeserializeOnTheFly() throws IOException {
   Schema schema = new Schema.Parser().parse(new File("src/test/resources/LogEntry2.avsc"));
   DatumReader&lt;GenericRecord&gt; datumReader = new GenericDatumReader&lt;&gt;(schema);
   File file = new File("/tmp/log2");
   DataFileReader&lt;GenericRecord&gt; dataFileReader = new DataFileReader&lt;&gt;(file, datumReader);
   GenericRecord entry = null;
   while (dataFileReader.hasNext()) {
      entry = dataFileReader.next(entry);
      System.out.println(entry);
   }
}
</code></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Avro Cookbook : Part I]]></title>
    <link href="http://bluesalt.github.io/blog/2014/02/06/avro-cookbook-part-i/"/>
    <updated>2014-02-06T15:49:00+08:00</updated>
    <id>http://bluesalt.github.io/blog/2014/02/06/avro-cookbook-part-i</id>
    <content type="html"><![CDATA[<ul id="markdown-toc">
  <li><a href="#initialize-the-project-structure">Initialize the project structure</a></li>
  <li><a href="#tweak-the-pomxml">Tweak the pom.xml</a></li>
  <li><a href="#import-the-project-into-intellij-idea">Import the project into Intellij IDEA</a></li>
  <li><a href="#recipe-2-define-a-schema">Recipe 2: Define a Schema</a></li>
  <li><a href="#recipe-3-serialize-the-log-data-to-disk-file">Recipe 3: Serialize the Log Data to Disk File</a></li>
  <li><a href="#recipe-4-deserialize-the-log-data-from-disk-file">Recipe 4: Deserialize the Log Data from Disk File</a></li>
</ul>

<p>Avro is a data serialization framework. It is an Apache project led by Doug Cutting who is also the author of several other open source projects such as Hadoop, Lucene. Recently I need to leverage Avro to serialize/deserialize some data, however, I found its document is too poor, at least too poor for newbies like me who don’t have much experience on data exchange format frameworks. </p>

<p>In fact, it is very easy to understand what Avro can do. It helps to convert Java objects into bytes and vice versa. The key information the framework needs to know is the format of the date, namely ‘Schema’ in Avro. In this article, I won’t spend any time on explaining what Avro is. 
<!--more-->
## Recipe 1: Create a Maven Avro Project
Intellij IDEA is my favorite Java IDE. The free Community edition has less features than the commercial Ultimate edition, however, great experience may be gained when the free community IDEA works with Maven. They complete each other. So the examples in this article will use Maven and Intellij IDEA as the IDE. Besides, <strong>TestNG</strong> instead of JUnit will be used as the test framework.</p>

<h3 id="initialize-the-project-structure">Initialize the project structure</h3>
<ul>
  <li>Create an project with quickstart archetype:</li>
</ul>

<p><code>
mvn archetype:generate -DgroupId=me.jeffli -DartifactId=avrosamples -Dversion=0.01 -DarchetypeArtifactId=maven-archetype-quickstart -DinteractiveMode=false
</code></p>

<h3 id="tweak-the-pomxml">Tweak the pom.xml</h3>
<ul>
  <li>Add Avro dependency:</li>
</ul>

<p>```xml</p>
<dependency>
   <groupid>org.apache.avro</groupid>
   <artifactid>avro</artifactid>
   <version>1.7.5</version>
</dependency>
<p>```</p>

<ul>
  <li>Use Avro Maven plugin</li>
</ul>

<p>```xml</p>
<plugin>
   <groupid>org.apache.avro</groupid>
   <artifactid>avro-maven-plugin</artifactid>
   <version>1.7.5</version>
   <executions>
      <execution>
         <phase>generate-sources</phase>
         <goals>
            <goal>schema</goal>
         </goals>
         <configuration>
            <!-- make sure the directory is created -->
            <sourcedirectory>${project.basedir}/src/main/avro/</sourcedirectory>
            <outputdirectory>${project.basedir}/src/main/java/</outputdirectory>
         </configuration>
      </execution>
   </executions>
</plugin>
<p>```</p>

<p>It should be noted that the directory <code>${project.basedir}/src/main/avro/</code> must be created even it is empty at first. It is used to place the Avro schema files. The whole pom.xml has been posted to <a href="https://gist.github.com/bluesalt/9807306">github gist</a>.</p>

<h3 id="import-the-project-into-intellij-idea">Import the project into Intellij IDEA</h3>
<p>IDEA provides full support to Maven, so it is very easy to import the Maven project as a IDEA project. Click “Import Project” in the ‘Quick Start’ panel. I suggest enable the Maven Auto-Import feature of IDEA before completing the importing process.</p>

<h2 id="recipe-2-define-a-schema">Recipe 2: Define a Schema</h2>
<p>Assume that you want to log every access of your server, to make it simple, we only define 3 attributes in a log entry, namely the username, resource and ip. So the schema can be defined as :</p>

<p><code>json
{
 "namespace": "me.jeffli.avrosamples.model",
 "type": "record",
 "name": "LogEntry",
 "fields": [
     {"name": "name", "type": "string"},
     {"name": "resource",  "type": ["string", "null"]},
     {"name": "ip", "type": ["string", "null"]}
 ]
}
</code></p>

<p>Save the content as <code>${project.basedir}/src/main/avro/LogEntry.avsc</code>. After running <code>mvn compile</code>, a Java class <code>me.jeffli.avrosamples.model.LogEntry</code> will be generated automatically thank to the Avro Maven plugin.  </p>

<h2 id="recipe-3-serialize-the-log-data-to-disk-file">Recipe 3: Serialize the Log Data to Disk File</h2>
<p>Assume we want to store the log data to a disk file <code>/tmp/log</code>. The code snippet would be like this:</p>

<p>```java
@Test
public void testSerializeLogEntries() throws IOException {
   LogEntry entry1 = new LogEntry();
   entry1.setName(“Jeff”);
   entry1.setResource(“readme.txt”);
   entry1.setIp(“192.168.1.1”);</p>

<p>LogEntry entry2 = new LogEntry();
   entry2.setName(“John”);
   entry2.setResource(“readme.md”);
   entry2.setIp(“192.168.1.2”);</p>

<p>DatumWriter<logentry> logEntryDatumWriter = new SpecificDatumWriter&lt;&gt;(LogEntry.class);
   DataFileWriter<logentry> dataFileWriter = new DataFileWriter&lt;&gt;(logEntryDatumWriter);
   File file = new File("/tmp/log");
   dataFileWriter.create(entry1.getSchema(), file);</logentry></logentry></p>

<p>dataFileWriter.append(entry1);
   dataFileWriter.append(entry2);</p>

<p>dataFileWriter.close();
}
```  </p>

<h2 id="recipe-4-deserialize-the-log-data-from-disk-file">Recipe 4: Deserialize the Log Data from Disk File</h2>
<p>Assume you need to parse the log data from disk files <code>/tmp/log</code>. Then the code snippet would be:</p>

<p><code>java
@Test (dependsOnMethods = "testSerializeLogEntries")
public void testDeSerializeLogEntries() throws IOException {
   DatumReader&lt;LogEntry&gt; logEntryDatumReader = new SpecificDatumReader&lt;&gt;(LogEntry.class);
   File file = new File("/tmp/log");
   DataFileReader&lt;LogEntry&gt; dataFileReader = new DataFileReader&lt;&gt;(file, logEntryDatumReader);
   LogEntry entry = null;
   while (dataFileReader.hasNext()) {
      entry = dataFileReader.next(entry);
      System.out.println(entry);
   }
}
</code>   </p>

]]></content>
  </entry>
  
</feed>
