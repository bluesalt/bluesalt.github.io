<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Tag: grpc | Jeff Li]]></title>
  <link href="http://bluesalt.github.io/tags/grpc/atom.xml" rel="self"/>
  <link href="http://bluesalt.github.io/"/>
  <updated>2017-09-04T23:18:59+08:00</updated>
  <id>http://bluesalt.github.io/</id>
  <author>
    <name><![CDATA[Jeff Li]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Keep Python gRPC  Client Connection Truly Alive]]></title>
    <link href="http://bluesalt.github.io/blog/2017/08/02/keep-python-grpc-client-connection-truly-alive/"/>
    <updated>2017-08-02T07:27:54+08:00</updated>
    <id>http://bluesalt.github.io/blog/2017/08/02/keep-python-grpc-client-connection-truly-alive</id>
    <content type="html"><![CDATA[<ul id="markdown-toc">
  <li><a href="#introduction">Introduction</a></li>
  <li><a href="#triage">Triage</a>    <ul>
      <li><a href="#reconnection-mechanism">Reconnection Mechanism</a></li>
      <li><a href="#code-revisit">Code Revisit</a></li>
    </ul>
  </li>
  <li><a href="#solution">Solution</a></li>
  <li><a href="#epilogue">Epilogue</a></li>
</ul>

<h3 id="introduction">Introduction</h3>
<p><a href="https://grpc.io/">gRPC</a> is a high performance RPC framework from Google which helps to reduce the gap between components within large scale systems. It provides a program paradigm in which code written by various languages is able to work together as a single language. The framework is promising due to the microservices  tide at the moment. </p>

<p>I have been working on a project which leverages gRPC recently. In the project, a component, the gRPC client, written with Python communicates with another one, the gRPC server every day like a daily cron job. And during each round, multiple calls will be issued by the client. </p>

<p>It worked well on the first round. One day later, when 2nd round got scheduled, unavailable exception was observed from the first call with the complaint.</p>

<!--more-->

<p><code>
_Rendezvous: &lt;_Rendezvous of RPC that terminated with (StatusCode.UNAVAILABLE,Endpoint read failed)&gt;
</code></p>

<p>What is more, succeeding calls were totally OK. </p>

<p>Fortunately, the issue is able to be reproduced by using the simple <a href="https://github.com/grpc/grpc/tree/master/examples/python/helloworld">hello world exmaple</a>. So I present a small project in <a href="https://github.com/bluesalt/pygrpc-reconnection">GitHub</a> based on the example to explain the detail. The demo program wraps a RPC client with object oriented style code.</p>

<ul>
  <li><a href="https://github.com/bluesalt/pygrpc-reconnection/commit/c12a3c2ab2e12de176f28919779cd48ef2aa13cf">Buggy code</a></li>
  <li><a href="https://github.com/bluesalt/pygrpc-reconnection/commit/b1f3ab21151f5d9ad4be611d19c143cebc2afcc2">Resolved code</a></li>
</ul>

<p>Check out the buggy code and reproduce the bug with the steps.</p>

<ol>
  <li>Start the server with <code>python server.py</code>.</li>
  <li>Start the client <code>python client.py</code> in another terminal and a normal greeting could see.</li>
  <li>Kill the server started in step 1 with <code>Ctrl + C</code> and then restart it within 30s. When next round is scheduled, the exception should be raised.</li>
</ol>

<h3 id="triage">Triage</h3>
<p>The issue is actually not rare. Through Google, one could find that there is even an <a href="https://github.com/grpc/grpc/issues/11043">official GitHub issue 11043</a> tracking the problem and that is exactly what I have met. However, it does not make any sense because the issue has been solved by <a href="https://github.com/grpc/grpc/pull/11154">PR 11154</a> and has been applied in our environment definitely. However, we are sure about 2 things at least.</p>

<ol>
  <li>The issue is caused by disconnected communication channel.</li>
  <li>The reconnection mechanism did not work as expected.</li>
</ol>

<h4 id="reconnection-mechanism">Reconnection Mechanism</h4>
<p>Every time an instance of <a href="https://github.com/grpc/grpc/blob/v1.4.2/src/python/grpcio/grpc/_channel.py#L912"><code>grpc._channel.Channel</code></a> is initialized, a <a href="https://github.com/grpc/grpc/blob/v1.4.2/src/python/grpcio/grpc/_channel.py#L932">subscription</a> which takes an callback parameter to monitor the connection status will be started. Multiple callbacks are allowed and every time the state of the connection changes, all of them will be called. According to the comment of the code, this is a workaround and will be removed in the future when c-core supports retry. The subscription actually takes following actions.</p>

<ol>
  <li><a href="https://github.com/grpc/grpc/blob/v1.4.2/src/python/grpcio/grpc/_channel.py#L880">Creates a thread</a> intended to poll the connection status if it does not exist yet.</li>
  <li><a href="https://github.com/grpc/grpc/blob/v1.4.2/src/python/grpcio/grpc/_channel.py#L880">Polling</a> the state of the connection.</li>
  <li>If there is no callback, <a href="https://github.com/grpc/grpc/blob/v1.4.2/src/python/grpcio/grpc/_channel.py#L854">quit the polling thread</a></li>
</ol>

<p>All the callbacks attached to the <code>grpc._channel.Channel</code> instance will be removed when the object is disposed by Python garbage collector because <a href="https://github.com/grpc/grpc/blob/v1.4.2/src/python/grpcio/grpc/_channel.py#L976">customized <code>__del__</code> method</a> which will detach all the callbacks is provided. That means connection state polling will be stopped which could cause our bug. Thus we are able to make an assumption now: the channel instance we have created somehow got destroyed by GC unexpectedly so that there is no connection monitor anymore.</p>

<h4 id="code-revisit">Code Revisit</h4>

<p>Let’s reexamine the buggy code.</p>

<p>```python
class RpcClient(object):
    def <strong>init</strong>(self):
        channel = grpc.insecure_channel(‘localhost:50051’)
        self.stub = helloworld_pb2_grpc.GreeterStub(channel)</p>

<pre><code>def say_hi(self ):
    response = self.stub.SayHello(helloworld_pb2.HelloRequest(name='you'))
    return response.message ```
</code></pre>

<p>Line 3 creates a channel instance that will be passed to initialize the client stub of type <a href="https://github.com/bluesalt/pygrpc-reconnection/blob/master/common/helloworld_pb2_grpc.py#L7"><code>GreeterStub</code></a> which is generated by the protocol buffer compiler. If we are able to find anything that proves that there is not reference to the channel instance anymore after some time, we get more close to the truth.</p>

<p>What is checked next is the generated code of <code>GreeterStub</code>.</p>

<p>```python
class GreeterStub(object):
  “"”The greeting service definition.
  “””</p>

<p>def <strong>init</strong>(self, channel):
    “"”Constructor.
    Args:
      channel: A grpc.Channel.
    “””
    self.SayHello = channel.unary_unary(
        ‘/helloworld.Greeter/SayHello’,
        request_serializer=helloworld__pb2.HelloRequest.SerializeToString,
        response_deserializer=helloworld__pb2.HelloReply.FromString,
        )
```  </p>

<p>Oh, there is no explicit reference to the channel instance in this class but it is insufficient to rule out implicit reference yet. The <a href="https://github.com/grpc/grpc/blob/v1.4.2/src/python/grpcio/grpc/_channel.py#L940"><code>grpc._channel.Channel.unary_unary</code></a> method requires more investigation. </p>

<p><code>python
def unary_unary(self,
                 method,
                 request_serializer=None,
                 response_deserializer=None):
     return _UnaryUnaryMultiCallable(
         self._channel,
         _channel_managed_call_management(self._call_state),
         _common.encode(method), request_serializer, response_deserializer)
</code></p>

<p>We can see that—although the whole channel instance is passed—the method references just some attributes of our channel instance but not the channel itself. That means after client stub is initialized, the channel instance will become an orphan Python object. So everything makes sense now.</p>

<h3 id="solution">Solution</h3>
<p>The solution is simple—prevent the channel instance from being collected.</p>

<p>```python
class RpcClient(object):
    def <strong>init</strong>(self):
        self.channel = grpc.insecure_channel(‘localhost:50051’)
        self.stub = helloworld_pb2_grpc.GreeterStub(self.channel)</p>

<pre><code>def say_hi(self ):
    response = self.stub.SayHello(helloworld_pb2.HelloRequest(name='you'))
    return response.message ```
</code></pre>

<h3 id="epilogue">Epilogue</h3>
<p>To be honest, the behavior is quite surprising. <code>__del__</code> is evil most of the time   and should be avoided unless you have to. We shall not blame the authors of gRPC because without this mechanism, resource leak occurs. And I believe this is the best solution so far.</p>

<p>This would be confusing for beginners because there is no doc about this. And almost all the code samples—even the unit test case in gRPC code base—use Python procedural programming style in which the bug would not be produced. A warning should have been documented. That is why this post is created.</p>

]]></content>
  </entry>
  
</feed>
